{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a53b17f9",
   "metadata": {},
   "source": [
    "Written by [Avihu Dekel](https://huggingface.co/Avihu).\n",
    "\n",
    "## Spoken Question Answering with Granite Speech’s Two-Pass Design\n",
    "\n",
    "[Granite speech](https://huggingface.co/collections/ibm-granite/granite-speech-67e45da088d5092ff6b901c7) is a family of powerful speech models, that excel in speech recognition and speech translation. Specifically, [granite-speech-3.3-8b](https://huggingface.co/ibm-granite/granite-speech-3.3-8b) leads the [OpenASR leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard) (as of June 2025).\n",
    "\n",
    "\n",
    "Granite Speech was trained by modality aligning [Granite](https://huggingface.co/collections/ibm-granite/granite-33-language-models-67f65d0cca24bcbd1d3a08e3) to speech. This was achieved by projecting the embeddings of a pretrained speech encoder to Granite's embedding space, and fine-tuning with using lightweight LoRA adapters. \n",
    "\n",
    "This enables a *two-pass design* with Granite Speech:\n",
    "1. *Transcribe* the audio using Granite Speech\n",
    "2. *Answer* the question within the transcription using Granite.\n",
    "\n",
    "Granite Speech supports both steps seamlessly. It automatically enables the LoRA adapters when audio input is detected, and disables them when processing text-only input.\n",
    "\n",
    "In this guide, we demonstrate how to use the two-pass design for spoken question answering.\n",
    "We'll showcase this using [LlamaQuestions](https://github.com/google-research-datasets/LLAMA1-Test-Set) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf51f0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/proj/mmllm/miniforge/envs/mma/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# spoken question answering dataset\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"fixie-ai/llama-questions\")[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c637cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████| 4/4 [00:04<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers.models.granite_speech import GraniteSpeechForConditionalGeneration, GraniteSpeechProcessor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model_name = \"ibm-granite/granite-speech-3.3-2b\"\n",
    "processor = GraniteSpeechProcessor.from_pretrained(model_name)\n",
    "model = GraniteSpeechForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=device)\n",
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58af86db",
   "metadata": {},
   "source": [
    "Let's write a simple function to transcribe a given audio input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80fd4e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the capital of france\n"
     ]
    }
   ],
   "source": [
    "def transcribe(audio) -> str:\n",
    "    system_prompt = \"Knowledge Cutoff Date: April 2024.\\nToday's Date: April 9, 2025.\\nYou are Granite, developed by IBM. You are a helpful AI assistant\"\n",
    "    instruction = \"Please transcribe the following audio to text<|audio|>\"\n",
    "    chat = [\n",
    "        dict(role=\"system\", content=system_prompt),\n",
    "        dict(role=\"user\", content=instruction)\n",
    "    ]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False,\n",
    "    )\n",
    "    model_inputs = processor(prompt, audio, device=device, return_tensors=\"pt\").to(device)\n",
    "    model_outputs = model.generate(**model_inputs, max_new_tokens=200)\n",
    "    num_input_tokens = model_inputs[\"input_ids\"].shape[-1]\n",
    "    new_tokens = model_outputs[:, num_input_tokens:]\n",
    "    output_text = tokenizer.batch_decode(\n",
    "        new_tokens, add_special_tokens=False, skip_special_tokens=True\n",
    "    )\n",
    "    return output_text\n",
    "\n",
    "transcription = transcribe(ds[0][\"audio\"][\"array\"])[0]\n",
    "print(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4e5c86",
   "metadata": {},
   "source": [
    "Now, let's use the base LLM (Granite 3.3 instruct) to answer the question. \n",
    "When the input contains only text (i.e., no audio), Granite Speech automatically *deactivates* the LoRA adapters and functions identically to the original Granite model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25833f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is Paris.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def llm_response(query):\n",
    "    chat = [dict(role=\"user\", content=query)]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        chat,\n",
    "        add_generation_prompt=True,\n",
    "        tokenize=False\n",
    "    )\n",
    "    model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    # calling the base LLM and disabling the LoRA adaptors\n",
    "    model_outputs = model.generate(**model_inputs, max_new_tokens=200)\n",
    "    num_input_tokens = model_inputs[\"input_ids\"].shape[-1]\n",
    "    new_tokens = model_outputs[:, num_input_tokens:]\n",
    "\n",
    "    output_text = tokenizer.batch_decode(\n",
    "        new_tokens, add_special_tokens=False, skip_special_tokens=True\n",
    "    )\n",
    "    return output_text\n",
    "\n",
    "llm_response(transcription)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e1d4c",
   "metadata": {},
   "source": [
    "Let’s run the full pipeline on a few more examples to see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18c327be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: what is the capital of france\n",
      "A: The capital of France is Paris.\n",
      "\n",
      "Q: which river is the longest in south america\n",
      "A: The longest river in South America is the Amazon River. It stretches approximately 6,992 kilometers (4,345 miles) long.\n",
      "\n",
      "Q: what is the highest mountain peak in north america\n",
      "A: The highest mountain peak in North America is Mount Denali (formerly known as Denali Mountain or North America's \"Great Mountain\"), located in the Alaska Range in Denali National Park and Preserve. It stands at a height of approximately 20,310 feet (6,190 meters) above sea level.\n",
      "\n",
      "Q: who was the first president of the united states\n",
      "A: The first president of the United States was George Washington. He served two terms from 1789 to 1797.\n",
      "\n",
      "Q: which city is located at the intersection of the tigris and euphrates rivers\n",
      "A: The city located at the intersection of the Tigris and Euphrates rivers is ancient Mesopotamia, specifically the city of Babylon. However, in modern times, no city exists at this exact location due to the changes in river courses over time.\n",
      "\n",
      "Q: what is the largest planet in our solar system\n",
      "A: The largest planet in our solar system is Jupiter. It's known for its Great Red Spot, a storm that has been raging on the planet for at least 300 years.\n",
      "\n",
      "Q: which country borders canada and mexico\n",
      "A: The United States borders both Canada and Mexico. Specifically, the United States shares its border with Canada to the north and with Mexico to the south.\n",
      "\n",
      "Q: what is the smallest country in the world\n",
      "A: The smallest country in the world by land area is Vatican City, with a size of just 0.44 square kilometers (0.17 square miles). It is an independent city-state enclaved within Rome, Italy.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    transcription = transcribe(ds[i][\"audio\"][\"array\"])[0]\n",
    "    response = llm_response(transcription)[0]\n",
    "    print(f\"Q: {transcription}\")\n",
    "    print(f\"A: {response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
